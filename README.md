# LLM-agent

The provided notebook explores building intelligent agents using Large Language Models (LLMs) and Vision-Language Models (VLMs) to reason about visual data and answer questions. The goal is to understand how prompting and architecture influence results and compare the performance of different approaches.

Core parts:
* Loading and exploring a visual question answering dataset
* Working with a powerful VLM (`QwenVLM`)
* Building a judge model to assess answer correctness
* Performing zero-shot evaluation of an LLM-based agent
* Designing and implementing classic and deep learning agents that can reason step-by-step
* Ablation studies to evaluate the impact of individual agents
* Comparing the performance of different approaches

  
The **goal** was to explore how prompting and architecture influence results and compare the performance of different approaches.

<br>
<br>

---

_Completing this notebook was part of the System 2 AI course ([sut-system2](https://sut-system2.github.io/)) I passed during my Master's program._
