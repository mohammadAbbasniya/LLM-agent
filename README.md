# LLM-agent

The provided notebook explores building intelligent agents using Large Language Models (LLMs) and Vision-Language Models (VLMs) to reason about visual data and answer questions. It covers loading a visual question answering dataset, working with a VLM, building a judge model to assess answer correctness, performing zero-shot evaluation, and designing and implementing classic and deep learning agents with intermediate reasoning steps. 

The **goal** was to explore how prompting and architecture influence results and compare the performance of different approaches.
